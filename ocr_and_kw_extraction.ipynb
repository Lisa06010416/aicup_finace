{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/llin07/Documents/GitHub/ai_cup_finace_rag/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from paddleocr import PaddleOCR, draw_ocr\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "\n",
    "from paddlex import create_pipeline\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "property = \"finance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change PDF to IMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdf_path_list = glob.glob(f'競賽資料集/reference/{property}/*')\n",
    "output_folder_prefix = f'pdf_images/{property}'\n",
    "\n",
    "for pdf_path in pdf_path_list:\n",
    "\n",
    "    file_name = os.path.basename(pdf_path)\n",
    "    output_folder = os.path.join(output_folder_prefix, file_name)\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    images = convert_from_path(os.path.join(pdf_path))\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = os.path.join(output_folder, f'page_{i+1}.jpg')\n",
    "        image.save(image_path, 'JPEG')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUsing official model (PP-OCRv4_mobile_det), the model files will be be automatically downloaded and saved in /Users/llin07/.paddlex/official_models.\u001b[0m\n",
      "\u001b[32mUsing official model (PP-OCRv4_mobile_rec), the model files will be be automatically downloaded and saved in /Users/llin07/.paddlex/official_models.\u001b[0m\n",
      "Processing directories: 100%|██████████| 1036/1036 [00:00<00:00, 1312779.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from paddlex import create_pipeline\n",
    "from opencc import OpenCC\n",
    "from tqdm import tqdm\n",
    "\n",
    "cc = OpenCC('s2t')  \n",
    "\n",
    "pipeline = create_pipeline(pipeline=\"OCR\")\n",
    "\n",
    "img_dir = f'pdf_images/{property}'\n",
    "\n",
    "output_text_dict = {}\n",
    "\n",
    "start_index = 750\n",
    "for i, (root, dirs, files) in enumerate(tqdm(list(os.walk(img_dir)), desc=\"Processing directories\")):\n",
    "    if i < start_index:\n",
    "        continue\n",
    "    files.sort()\n",
    "    for file in files:\n",
    "        if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_path = os.path.join(root, file)\n",
    "            \n",
    "            parent_folder = os.path.basename(os.path.dirname(img_path))\n",
    "\n",
    "            if parent_folder not in output_text_dict:\n",
    "                output_text_dict[parent_folder] = []\n",
    "                print(parent_folder)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            output = pipeline.predict(img_path)\n",
    "            for res in output:\n",
    "                traditional_text = [cc.convert(text) for text in res[\"rec_text\"]]\n",
    "                output_text_dict[parent_folder].append({\"text\":traditional_text, \"input_path\":res[\"input_path\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR results saved to output/ocr_finance_output.json\n"
     ]
    }
   ],
   "source": [
    "output_file_path = f\"output/ocr_{property}_output.json\"\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_text_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"OCR results saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert ocr result to mackdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"output/ocr_{property}_output.json\", 'r', encoding='utf-8') as f:\n",
    "    ocr_output_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []\n",
    "file_paths = [] \n",
    "\n",
    "for documents in ocr_output_dict:\n",
    "    for page in ocr_output_dict[documents]:\n",
    "        text_list = page[\"text\"]\n",
    "        text_path = page[\"input_path\"].replace('pdf_images/', 'pdf_md/').replace('.jpg', '.md')\n",
    "        \n",
    "    \n",
    "\n",
    "        prompt = (\n",
    "                        f\"Convert the OCR results into Markdown format. \"\n",
    "                        f\"The input is a list of text extracted by the OCR model, containing both free text and tables from {property}-related documents. \"\n",
    "                        f\"Please transform this list into Markdown format, correcting any typos you find.\\n\"\n",
    "                        f\"Output the result in JSON format, like this: {{'markdown': 'generate markdown content here'}}.\\n\"\n",
    "                        f\"Input text: {text_list}\\n\"\n",
    "                    )\n",
    "        \n",
    "        task = {\n",
    "            \"custom_id\": text_path,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"temperature\": 0.1,\n",
    "                \"response_format\": { \n",
    "                    \"type\": \"json_object\"\n",
    "                },\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        tasks.append(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"batch_tasks_ocr2md.jsonl\"\n",
    "\n",
    "with open(file_name, 'w') as file:\n",
    "    for obj in tasks:\n",
    "        file.write(json.dumps(obj) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_file = client.files.create(\n",
    "  file=open(file_name, \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")\n",
    "print(batch_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_job = client.batches.create(\n",
    "  input_file_id=batch_file.id,\n",
    "  endpoint=\"/v1/chat/completions\",\n",
    "  completion_window=\"24h\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get result from onenapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_6717a41d61f881909087b5ab379ee8cb', completion_window='24h', created_at=1729602589, endpoint='/v1/chat/completions', input_file_id='file-PEWgHdaExlbbwYX77ZPZ8B3P', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1729608922, error_file_id='file-ixliCTzQoOKUZww8adXlHKeU', errors=None, expired_at=None, expires_at=1729688989, failed_at=None, finalizing_at=1729607701, in_progress_at=1729602595, metadata=None, output_file_id='file-bnGjb49yAOuLSyhcnoUSnROd', request_counts=BatchRequestCounts(completed=4473, failed=2, total=4475))\n"
     ]
    }
   ],
   "source": [
    "batch_job = client.batches.retrieve(batch_job.id)\n",
    "print(batch_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_id = batch_job.output_file_id\n",
    "result = client.files.content(result_file_id).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_name = \"batch_job_results_ocr2md.jsonl\"\n",
    "\n",
    "with open(result_file_name, 'wb') as file:\n",
    "    file.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_markdown(s):\n",
    "    s = s.strip(\"'\").rstrip(\",\").strip()\n",
    "    try:\n",
    "        data = json.loads(s)\n",
    "        markdown_content = data.get('markdown', '')\n",
    "        markdown_content = markdown_content.replace('\\\\n', '\\n')\n",
    "        return markdown_content\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"解析 JSON 時發生錯誤：\", e)\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解析 JSON 時發生錯誤： Unterminated string starting at: line 2 column 15 (char 16)\n",
      "解析 JSON 時發生錯誤： Unterminated string starting at: line 2 column 15 (char 16)\n",
      "Processed and saved 4473 files.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "with open(result_file_name, 'r') as file:\n",
    "    for line in file:\n",
    "        json_object = json.loads(line.strip())\n",
    "\n",
    "        file_path = json_object[\"custom_id\"]\n",
    "        try:\n",
    "            markdown_content = json.loads(json_object[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"])[\"markdown\"]\n",
    "        except:\n",
    "            markdown_content = extract_markdown(json_object[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        os.makedirs(os.path.dirname(dir_path), exist_ok=True)\n",
    "        \n",
    "        with open(file_path, 'w', encoding='utf-8') as md_file:\n",
    "            md_file.write(markdown_content)\n",
    "\n",
    "        results.append({\n",
    "            'file_path': file_path,\n",
    "            'markdown_content': markdown_content\n",
    "        })\n",
    "\n",
    "print(f\"Processed and saved {len(results)} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extraction keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "tasks = []\n",
    "file_paths = [] \n",
    "\n",
    "# Define the directory path\n",
    "pdf_md_finance_dir = 'pdf_md/finance/*'\n",
    "\n",
    "# Read all files in the pdf_md/finance directory\n",
    "file_paths = glob.glob(os.path.join(pdf_md_finance_dir, '*'))\n",
    "\n",
    "for file in file_paths:\n",
    "\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    keyword_extraction_prompt = (\n",
    "        f\"Please extract keywords from the input text, which may be in English or Traditional Chinese. \"\n",
    "        f\"The input text could be a sentence or content in Markdown format. \"\n",
    "        f\"Extract the keywords and classify them into the following categories: Person, Place, Company, Time, or {property} terminology.\"\n",
    "        f\"Category Descriptions:\"\n",
    "        f\"Person: Names of individuals\"\n",
    "        f\"Place: e.g., Taipei\"\n",
    "        f\"Company: Names of companies in Taiwan, e.g., 長榮, 亞德客-KY, or 光寶科技股份有限公司\"\n",
    "        f\"Time: e.g., 2022年第3季\"\n",
    "        f\"{property} terminology: Specific terms, e.g., 要保人, 受益人, 綜合損益總額, 淨現金流出, 合併權益變動表, 資產總額, 合併資產總額, 營業利益\"\n",
    "        f\"\\nPlease output in JSON format, like this example: {{'Person': [], 'Place': [], 'Company': ['長榮'], 'Time': [], '{{property}} terminology': ['綜合損益總額']}}\"\n",
    "        f\"The input text is {text}\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    task = {\n",
    "            \"custom_id\": file,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"temperature\": 0.1,\n",
    "                \"response_format\": { \n",
    "                    \"type\": \"json_object\"\n",
    "                },\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": keyword_extraction_prompt\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    tasks.append(task)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split tasks into 3 parts\n",
    "num_tasks = len(tasks)\n",
    "chunk_size = num_tasks // 3\n",
    "chunks = [tasks[i:i + chunk_size] for i in range(0, num_tasks, chunk_size)]\n",
    "\n",
    "# Ensure we have exactly 3 chunks by adjusting the last chunk if necessary\n",
    "if len(chunks) > 3:\n",
    "    chunks[2].extend(chunks[3:])\n",
    "    chunks = chunks[:3]\n",
    "\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    file_name = f\"batch_tasks_md2keyword_part{i}.jsonl\"\n",
    "    with open(file_name, 'w') as file:\n",
    "        for obj in chunk:\n",
    "            file.write(json.dumps(obj) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-pEXNrFf9skPR8aQyvKK9EEOE', bytes=7053968, created_at=1729849706, filename='batch_tasks_md2keyword_part3.jsonl', object='file', purpose='batch', status='processed', status_details=None)\n"
     ]
    }
   ],
   "source": [
    "file_name = \"batch_tasks_md2keyword_part3.jsonl\"\n",
    "\n",
    "batch_file = client.files.create(\n",
    "  file=open(file_name, \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")\n",
    "print(batch_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_job = client.batches.create(\n",
    "  input_file_id=batch_file.id,\n",
    "  endpoint=\"/v1/chat/completions\",\n",
    "  completion_window=\"24h\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'batch_671b696ca2288190830d764842fcbf36'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_job.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_671b696ca2288190830d764842fcbf36', completion_window='24h', created_at=1729849708, endpoint='/v1/chat/completions', input_file_id='file-pEXNrFf9skPR8aQyvKK9EEOE', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1729851327, error_file_id=None, errors=None, expired_at=None, expires_at=1729936108, failed_at=None, finalizing_at=1729850622, in_progress_at=1729849709, metadata=None, output_file_id='file-6Ab7dKU2m4ztk5fHw3R5jhBQ', request_counts=BatchRequestCounts(completed=1491, failed=0, total=1491))\n"
     ]
    }
   ],
   "source": [
    "batch_job = client.batches.retrieve(batch_job.id)\n",
    "print(batch_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_id = batch_job.output_file_id\n",
    "result = client.files.content(result_file_id).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_name = \"batch_job_results3_md2kw.jsonl\"\n",
    "\n",
    "with open(result_file_name, 'wb') as file:\n",
    "    file.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved 1035 files.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "split_file_list = [\"batch_job_results1_md2kw.jsonl\", \"batch_job_results2_md2kw.jsonl\", \"batch_job_results3_md2kw.jsonl\"]\n",
    "\n",
    "results = defaultdict(list)\n",
    "for result_file_name in split_file_list:\n",
    "    with open(result_file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            json_object = json.loads(line.strip())\n",
    "\n",
    "            file_path = json_object[\"custom_id\"]\n",
    "            try:\n",
    "                json_content = json.loads(json_object[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"])\n",
    "            except:\n",
    "                print(f\"ERROR to conver {file_path}\")\n",
    "\n",
    "            dir_path = os.path.dirname(file_path)\n",
    "\n",
    "            results[os.path.dirname(file_path).replace('pdf_md/', '')].append({\n",
    "                'page': os.path.basename(file_path),\n",
    "                'kw': json_content\n",
    "            })\n",
    "\n",
    "print(f\"Processed and saved {len(results)} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to output/finance_kw_extraction.json\n"
     ]
    }
   ],
   "source": [
    "# Save the results dictionary as a JSON file\n",
    "output_file = 'output/finance_kw_extraction.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction Query KW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '競賽資料集/dataset/preliminary/questions_example.json'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    questions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': 1,\n",
       " 'source': [442, 115, 440, 196, 431, 392, 14, 51],\n",
       " 'query': '匯款銀行及中間行所收取之相關費用由誰負擔?',\n",
       " 'category': 'insurance'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[\"questions\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "tasks = []\n",
    "file_paths = [] \n",
    "\n",
    "# Define the directory path\n",
    "pdf_md_finance_dir = 'pdf_md/finance/*'\n",
    "\n",
    "# Read all files in the pdf_md/finance directory\n",
    "file_paths = glob.glob(os.path.join(pdf_md_finance_dir, '*'))\n",
    "\n",
    "for data in questions[\"questions\"]:\n",
    "\n",
    "    text = data[\"query\"]\n",
    "    \n",
    "    keyword_extraction_prompt = (\n",
    "        f\"Please extract keywords from the input text, which may be in English or Traditional Chinese. \"\n",
    "        f\"The input text could be a sentence or content in Markdown format. \"\n",
    "        f\"Extract the keywords and classify them into the following categories: Person, Place, Company, Time, or {property} terminology.\"\n",
    "        f\"Category Descriptions:\"\n",
    "        f\"Person: Names of individuals\"\n",
    "        f\"Place: e.g., Taipei\"\n",
    "        f\"Company: Names of companies in Taiwan, e.g., 長榮, 亞德客-KY, or 光寶科技股份有限公司\"\n",
    "        f\"Time: e.g., 2022年第3季\"\n",
    "        f\"{property} terminology: Specific terms, e.g., 要保人, 受益人, 綜合損益總額, 淨現金流出, 合併權益變動表, 資產總額, 合併資產總額, 營業利益\"\n",
    "        f\"\\nPlease output in JSON format, like this example: {{'Person': [], 'Place': [], 'Company': ['長榮'], 'Time': [], '{{property}} terminology': ['綜合損益總額']}}\"\n",
    "        f\"The input text is {text}\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    task = {\n",
    "            \"custom_id\": str(data[\"qid\"]),\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"temperature\": 0.1,\n",
    "                \"response_format\": { \n",
    "                    \"type\": \"json_object\"\n",
    "                },\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": keyword_extraction_prompt\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    tasks.append(task)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"question2kw_batch.jsonl\"\n",
    "\n",
    "with open(file_name, 'w') as file:\n",
    "    for obj in tasks:\n",
    "        file.write(json.dumps(obj) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-lRpYDjSe88bO8ovyv2S8hHmL', bytes=208652, created_at=1729952106, filename='question2kw_batch.jsonl', object='file', purpose='batch', status='processed', status_details=None)\n"
     ]
    }
   ],
   "source": [
    "batch_file = client.files.create(\n",
    "  file=open(file_name, \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")\n",
    "print(batch_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_job = client.batches.create(\n",
    "  input_file_id=batch_file.id,\n",
    "  endpoint=\"/v1/chat/completions\",\n",
    "  completion_window=\"24h\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_671cf96c72008190b0a2d95f05e1e314', completion_window='24h', created_at=1729952108, endpoint='/v1/chat/completions', input_file_id='file-lRpYDjSe88bO8ovyv2S8hHmL', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1729952186, error_file_id=None, errors=None, expired_at=None, expires_at=1730038508, failed_at=None, finalizing_at=1729952173, in_progress_at=1729952108, metadata=None, output_file_id='file-hUqKbSLBOTqWQXBF9VKBvvOk', request_counts=BatchRequestCounts(completed=150, failed=0, total=150))\n"
     ]
    }
   ],
   "source": [
    "batch_job = client.batches.retrieve(batch_job.id)\n",
    "print(batch_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_id = batch_job.output_file_id\n",
    "result = client.files.content(result_file_id).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_name = \"question2kw.jsonl\"\n",
    "\n",
    "with open(result_file_name, 'wb') as file:\n",
    "    file.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved 150 files.\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "with open(result_file_name, 'r') as file:\n",
    "    for line in file:\n",
    "        json_object = json.loads(line.strip())\n",
    "\n",
    "        custom_id = json_object[\"custom_id\"]\n",
    "        try:\n",
    "            content = json.loads(json_object[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"])\n",
    "        except:\n",
    "            print(f\"Error convert json {custom_id}\")\n",
    "\n",
    "\n",
    "\n",
    "        results[custom_id] = content\n",
    "\n",
    "print(f\"Processed and saved {len(results)} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to output/finance_question_kw.json\n"
     ]
    }
   ],
   "source": [
    "with open('output/finance_question_kw.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Results saved to output/finance_question_kw.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
